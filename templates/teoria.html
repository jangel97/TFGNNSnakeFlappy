<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Clean Blog - Start Bootstrap Theme</title>

  <!-- Bootstrap core CSS -->
  <link href="/static/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="/static/vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <!-- Custom styles for this template -->
  <link href="/static/css/clean-blog.min.css" rel="stylesheet">

</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand" href="{{ url_for('index') }}">Neuronal Networks</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
	  <li class="nav-item">
            <a class="nav-link" href="{{ url_for('resumen') }}">Resumen</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="{{ url_for('teoria') }}">Teoria</a>
          </li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#">Librerias</a>
		<div class="dropdown-menu">
			<a class="dropdown-item" href="{{ url_for('libTfl') }}">TFLearn</a>
			<a class="dropdown-item" href="{{ url_for('libKeras') }}">Keras</a>
			<a class="dropdown-item" href="{{ url_for('comparativaLibs') }}">Comparativa Librerias</a>
		</div>
          </li>
	  <li class="nav-item">
            <a class="nav-link" href="{{ url_for('framework') }}">Framework</a>
          </li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#">Juegos</a>
		<div class="dropdown-menu">
			<a class="dropdown-item" href="{{ url_for('tfl') }}">Snake TFLearn</a>
			<a class="dropdown-item" href="{{ url_for('keras') }}">Snake Keras</a>
			<a class="dropdown-item" href="{{ url_for('flappy') }}">Flappy Bird</a>	
		</div>
          </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!-- Page Header -->
  <header class="masthead" style="background-image: url('/static/img/teoria.jpg')">
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="page-heading">
            <h1>Teoria</h1>
            <span class="subheading">Como funciona una red neuronal?</span>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- Main Content -->
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto" style="text-align: justify">
        <p>Durante estos últimos años, el término deep learning, ha tomado mucho protagonismo en la comunidad tecnológica informática a nivel mundial. Sin embargo, poco se sabe de esta nueva técnica y los mecanismos que hacen que funcione por debajo de cualquier abstracción o librería de código. En apartado tratará de: </p>
	<ul>
		<li>Explicar el funcionamiento de una red neuronal.</li>
		<li>Entender cómo funciona el mecanismo de aprendizaje.</li>
		<li>Aclarar los tipos de redes neuronales más básicos pero principales.</li>
		<li>Puntualizar las tecnologías más punteras actualmente en este área.</li>
		<ul>
			<li>Estudiar las tecnologías con mayor comunidad.</li>
			<li>Elegir tecnología para el proyecto.</li>
			<li>Contrastar herramientas a emplear.</li>
		</ul>
	</ul>
        <p>El objetivo es transmitir una idea aproximada y precisa de cómo funciona esta técnica y fomentar el entendimiento fundamental de esta con tal de propiciar un mayor entendimiento del trabajo y las decisiones tomadas en este.</p>
        <p>Pero antes de empezar con los puntos mencionados anteriormente tenemos que explicar que es el machine learning y el deep learning con un poco de profundidad.</p>

	<h2 class="section-heading" id="machine">¿Qué es el machine learning?</h2>

	<p>El <b>machine learning</b> (aprendizaje automático, en español) es una rama de la inteligencia artificial, con el objetivo de desarrollar técnicas para que los ordenadores <i>aprendan</i>. Explicado de una manera más precisa, se trata de <i>programas/algoritmos</i> que tienen la capacidad de generar comportamientos a partir de <i>datasets</i> en forma de ejemplos, de cuya información se alimentan para procesamiento de información. </p>
	<p>El aprendizaje automático puede ser de diferentes tipos:</p>
	<ul>
		<li><b>Aprendizaje supervisado:</b> sirve para deducir una función a partir de datos de entrenamiento. Los datos de entrenamiento son el dataset que en este caso está formado por los datos de entrada al sistema y los resultados deseados. La salida puede ser una valor numérico (para los problemas de regresión) o una etiqueta de clase (para los de clasificación).</li>
		<li><b>Aprendizaje no supervisado:</b> este ajusta un modela a las observaciones que se le han dado. Se diferencia del aprendizaje supervisado por no tener un dataset a priori. Así trata los objetos de entrada como variables aleatorias construyendo un modelo de densidad para el conjunto de datos. </li>
		<li><b>Aprendizaje por refuerzo:</b> su labor es determinar qué acciones debe elegir un agente de software en un entorno dado con el fin de maximizar alguna noción de recompensa o premio acumulado. </li>
		<li><b>Transducción:</b> son similares a los supervisados, pero no deducen una función sino que intentan predecir los futuros ejemplos basándose en los ejemplos de entrada y ejemplos de nuevos sistemas.Transducción: son similares a los supervisados, pero no deducen una función sino que intentan predecir los futuros ejemplos basándose en los ejemplos de entrada y ejemplos de nuevos sistemas.</li>
		
	</ul>
	<p>El mundo del machine learning tiene diferentes técnicas de clasificación como podrían ser los árboles de decisiones y algoritmos genéricos pero en este trabajo nos vamos a centrar en las redes neuronales artificiales (Artificial Neuronal Networks, en Inglés) y el deep learning. </p>

	<h2 class="section-heading" id="deep">¿Qué es el deep learning?</h2>

	<p>El <b>deep learning</b> (aprendizaje profundo, en español) está inspirado en la manera de procesar la información y los patrones de comunicación que encontramos en los sistemas nerviosos biológicos pero hay diferencias entre las propiedades estructurales y funcionales de los cerebros biologicos. </p>
	<p>El aprendizaje profundo es una parte del amplio mundo de del machine learning basado en métodos de aprendizaje de representación. Estos pueden ser <i>supervisados, semi-supervisados y no supervisados</i>, que son los tipos de algoritmos explicados anteriormente. Estos algoritmos son usados para formar la base del conexionismo que ha su vez es la base de las Artificial Neuronal Networks.</p>

	<center>
	  	<img class="img-fluid" src="/static/img/simple.png">
	 </center>
	 <span class="caption text-muted"> Ejemplo network+hidden layer</span>

	<p>El conexionismo es un enfoque en el campo de la ciencia cognitiva con la esperanza de poder explicar los fenómenos mentales utilizados en las redes neuronales artificiales. Su principal objetivo es que los fenómenos mentales puedan ser descritos por redes de unidades sencillas y frecuentemente iguales que se interconectan. La forma de las conexiones y de las unidades varía de un modelo a otro. Por ejemplo, las unidades de la red podrían representar neuronas y las conexiones podrían representar sinapsis. Otro modelo podría hacer cada unidad de la red una palabra, y cada conexión una indicación de similitud semántica. Las redes neuronales artificiales son modelos conexionistas y hay una gran variedad de modelos de redes neuronales, aunque casi siempre siguen dos principios básicos relativos a la mente: </p>

	<ul>
		<li>Cualquier estado mental puede ser descrito como un vector de (N)dimensional.</li>
		<li>La memoria se crea cuando se modifican los valores que representan la fuerza de las conexiones entre las unidades neuronales. La fuerza de las conexiones son generalmente representadas como una matriz de (N x N) dimensiones.</li>
	</ul>
	<p>Una vez explicadas estas bases podemos pasar a explicar que es una Artificial Neuronal Network y cómo funciona.</p>

	<h2 class="section-heading" id="ann">¿Qué es una Artificial Neuronal Network?</h2>

	<p>Una <b>Red Neuronal Artificial</b> es un intento minimalista de modelo computacional sobre el comportamiento cerebral y sistema nervioso. Como en el sistema nervioso, la unidad básica fundamental es la neurona.</p>
	<p>Para poder predecir y tener funcionamiento, las redes neuronales deben ser entrenadas. Este entrenamiento puede ser tanto de manera <i>supervisada, semi-supervisada o no supervisada</i>. En este trabajo, el entrenamiento implementado será supervisado para los distintos programas. El objetivo de este tipo de entrenamiento es capacitar la red neuronal para dada una entrada, tener la salida deseada. Su funcionamiento se observará más en profundidad posteriormente.</p>

	<h4 class="section-heading-2">Modelo biológico vs modelo artificial</h4>

	<p>La teoría y modelado de las redes neuronales artificiales se inspira en el funcionamiento de los sistemas nerviosos. En estos la neurona es el elemento fundamental. </p>

	<center>
	  	<img class="img-fluid" src="/static/img/neurona.png">
	 </center>
	 <span class="caption text-muted"> Neurona Biológica</span>

	<p>El axón de las neuronas, se alarga hasta conectarse con las dendritas. Estas son las encargadas de establecer conexión con otras neuronas. Esto se conoce como sinapsis.</p>
	<p>La sinapsis es una aproximación (funcional) intercelular especializada entre neuronas. En estos contactos se lleva a cabo la transmisión del impulso nervioso. Esto se inicia con una descarga química que origina una corriente eléctrica en la membrana de la célula emisora; una vez se da la comunicación con la neurona receptora, ésta segrega una sustancia encargada de excitar o inhibir la la acción de la anterior.</p>
	<p>Sinapsis proviene de sinapteína, cuyo orígen etimológico es sin- “juntos” y hapteina “con firmeza” Cada neurona, al igual que en las redes neuronales artificiales, puede establecer conexión desde con una docena de neuronas hasta con cientos de miles. </p>

	<p>Con tal de aclarar las ideas y establecer una similitud directa entre la actividad sináptica y la actividad análoga a las redes neuronales artificiales. Las señales que llegan a la sinapsis son la entradas a la neurona; estas son ponderadas (atenuadas o simplificadas) a través de un peso ponderado, asociado a la sinapsis correspondiente. Estas señales pueden excitar a la neurona o ser inhibida (peso negativo). El efecto es la suma de las entradas ponderadas. Si la suma es igual o mayor que el umbral de la neurona, entonces esta se activa, tal como en la sinapsis neuronal.</p>
	<p>Cada neurona se representa mediante: entradas (dendritas), funciones de activación y salidas(axones).</p>
	<p>Esquema de una neurona: </p>

	<center>
	  	<img class="img-fluid" src="/static/img/neurona_artificial.jpg">
	 </center>
	 <span class="caption text-muted"> Neurona Artificial</span>

	<p>Como se puede ver, podríamos definir una red neuronal como una función composición. En cuanto a la arquitectura básica de esta, se pueden diferenciar 3 componentes principales: </p>
	
	<ul>
		<li><b>Neuronas:</b> Estas contienen las diferentes ponderaciones y operaciones que hacer con los datos de entrada. La función de activación (o de mapeo) sirve para restringir los valores de los datos a un rango. Algunas funciones de activación son, sigmoidal, limitante, escalera, gaussiana, lineal, tangente hiperbólica.</li>

		<center>
		  	<img class="img-fluid" src="/static/img/neurona_artificial.png">
		</center>
		<span class="caption text-muted"> Gráficas de funciones de activación</span>

		<li><b>Conexiones/Pesos ponderados:</b> Valores de la ponderación de cada conexión. </li>
		<li><b>Biases:</b> Corresponden a los valores tras multiplicar los distintos pesos de la red frente a los datos de entrada. Estos en un principio no son correctos, es la red la que mediante back-propagation los irá aproximando a los valores óptimos</li>
	</ul>
	<p><b>Back-propagation</b>, es el algoritmo mediante el cual la red neuronal aproxima los valores de los pesos ponderados. Para conseguirlo, se calcula una salida deseada, se compara con la que tendría que ser (aprendizaje supervisado) y se trata de corregir las ponderaciones para el cálculo óptimo de los biases. </p>
	<ul>
		<li>Capas: Las capas, ayudan a incrementar la no linealidad de las salidas de la red neuronal. Cada capa contiene una cantidad de neuronas. Se pueden identificar las siguientes:</li>
		<ul>
			<li><b>Input layer:</b> Esta es la primera capa de la red. Hay tantas neuronas, como factores relevantes consideremos para la red. Es decir, un factor es relevante cuando es algo a tener en cuenta por la red. Por ejemplo, si se quisiera implementar pequeña red capaz de jugar al juego de la serpiente, los factores relevantes serían, el movimiento realizado (en una jugada), si se ha impactado con la comida o no, si se ha acabado la partida, etc. Por cada uno de estos, habrá una neurona en la input layer. </li>
			<li><b>Hidden layer:</b> Estas capas, se emplean para añadir no linealidad a la red y precisión. Es importante tener en cuenta que si se quiere hacer algo bastante simple, no se requiere el uso de las hidden layers. Se pueden añadir tantas como uno quiera y su función de activación puede ser diferente a la del resto de capas. </li>
			<li><b>Output layer:</b> En esta capa se encuentran tantas neuronas como posibles salidas esperamos de una predicción.</li>
		</ul>
	</ul>

	<center>
		  <img class="img-fluid" src="/static/img/neurona_artificial.png">
	</center>
	<span class="caption text-muted"> Gráficas de funciones de activación</span>

	<h4 class="section-heading-2">Proceso de Aprendizaje</h4>

	<p>Entrenar una red neuronal, mediante los pesos ponderados y los biases, implica un proceso de <i>ida y vuelta</i>, un proceso iterativo. La ida se conoce como <b>forwardpropagation</b>, mientras que la vuelta corresponde a <i>backpropagation</i>. </p>
	<center>
		  <img class="img-fluid" src="/static/img/neurona_artificial.png">
	</center>
	<span class="caption text-muted"> Gráficas de funciones de activación</span>

	<p>La primera fase es la <i>forward propagation</i>, esta ocurre cuando la red se expone a un <i>dataset</i> de entrenamiento y se procede a calcular las distintas ponderaciones. En resumen, pasar toda la información por la red, para que cada una de las neuronas aplique su función y la información vaya pasando de capa en capa, hasta llegar a la última capa cuya salida es el resultado de la red. Al ser entrenamiento supervisado, se aplica una una función para estimar el error de la predicción y medir como de correcta/incorrecta ha sido en relación al resultado esperado. Idealmente el error debiera ser cero, cuanto más entrene la red, mejores predicciones y por tanto este error tenderá más rápidamente a cero.</p>
	<p>La siguiente fase es la <i>backpropagation</i>, una vez se ha calculado el error en la predicción (durante el entrenamiento), el error se propaga hacia atrás, desde la output layer pasando por las hidden layers (estas solo reciben una fracción dependiendo de la contribución relativa de cada neurona en el resultado final), hasta volver a las input layers. Recalculando cada una de las ponderaciones.</p>
	<p>Este proceso, se repite iterativamente, hasta que se el error en la predicción es prácticamente cero. Esta técnica se conoce como <b>gradiente descendiente</b>. Este nombre proviene por el hecho que los pesos ponderados de las conexiones se van modificando con pequeños incrementos con la ayuda del cálculo de la derivada de la función que estima el error en la predicción de la red neuronal. La cual ayuda a calcular los pesos ponderados.</p>
	<p>El proceso de aprendizaje podría verse de la siguiente manera: </p>
	
	<center>
		  <img class="img-fluid" src="/static/img/neurona_artificial.png">
	</center>
	<span class="caption text-muted"> Gráficas de funciones de activación</span>

	<p>A continuación se presentarán más en detalle cada uno de los elementos presentados.</p>

	<h4 class="section-heading-2">Funciones de activación</h4>

	<p>Las <b>funciones de activación</b> son usadas para propagar la salida de una neurona hacia adelante. Esta salida es recibida por las neuronas de la siguiente capa a la que está conectada está neurona. Estas funciones de activación sirven para introducir la no linealidad entre las capas de la red neuronal. </p>
	<p>A continuación, vamos a entrar más en detalle con las 4 funciones más usadas hoy en día en <i>Deep Learning.</i></p>
	
	<h6 class="section-heading-4">Funciones Lineal</h6>

	<p>Como su propio nombre indica la función <i>Linear</i> no cambia la señal de salida. Se suele usar como función de activación de la <i>Input Layer</i> para que lleguen los datos hacia las <i>Hidden Layer</i> lo menos modificado possible.</p>

	<center>
		  <img class="img-fluid" src="/static/img/neurona_artificial.png">
	</center>
	<span class="caption text-muted"> Gráficas de funciones de activación</span>

	<h6 class="section-heading-4">Funciones Sigmoid</h6>

	<p>Esta función nos permite hacer una reducción de valores extremos o atípicos en datos válidos sin eliminarlos. En otras palabras más simples, esta función convierte variables independientes de rango casi infinito a probabilidades simples entre 0 y 1.</p>

	<center>
		  <img class="img-fluid" src="/static/img/neurona_artificial.png">
	</center>
	<span class="caption text-muted"> Gráficas de funciones de activación</span>

	<h6 class="section-heading-4">Funciones Tanh</h6>

	<p>La función Tanh, también llamada tangente hiperbólica, representa gráficamente la relación entre el seno hiperbólico y el coseno hiperbólico tanh(x)=sinh(x)/cosh(x). A diferencia de la función Sigmoid, el rango de normalización de Tanh es entre -1 y 1. El aspecto positivo del Tanh respecto Sigmoid es que los números negativos se pueden tratar con más facilidad. </p>

	<center>
		  <img class="img-fluid" src="/static/img/neurona_artificial.png">
	</center>
	<span class="caption text-muted"> Gráficas de funciones de activación</span>

	<h6 class="section-heading-4">Funciones ReLU</h6>

	<p>La función llamada unidad lineal rectificada, también llamada coloquialmente función rampa activa solo un nodo si la entrada está por encima de un cierto umbral. El comportamiento que tiene dicha función es siempre que la entrada sea inferior a cero la salida será cero y si la entrada es superior la salida será en relación lineal con la variable de entrada.</p>

	<center>
		  <img class="img-fluid" src="/static/img/neurona_artificial.png">
	</center>
	<span class="caption text-muted"> Gráficas de funciones de activación</span>

	<h4 class="section-heading-2">Optimizadores</h4>

	<p>Los optimizadores son argumentos requeridos en el momento en que se hace la función <i>complie()</i> solo podemos utilizar un optimizador en el entrenamiento de la red neuronal. Las posibilidades que tenemos son las siguientes: SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax i Nadam. Adam es el más efectivo en menos tiempo de entrenamiento. </p>

	<h4 class="section-heading-2">Parametrización de modelo</h4>

	<p>Los modelos generalmente suelen tener los siguientes parámetros:</p>
	<ul>
		<li>Epochs.</li>
		<li>Número de neuronas por capa.</li>
		<li>Número de capas.</li>
		<li>Función de activación.</li>
	</ul>
	<p>Los epochs, indican el número de veces que el dataset en cuestión pasa sobre la red neuronal en el proceso de aprendizaje. Incrementar el número de epochs es interesante para incrementar la precisión de la red neuronal siempre teniendo en cuenta un posible problema de overfitting.</p>
	<p>Estos parámetros son fundamentales para poder incrementar el rendimiento de la red neuronal y el tiempo de ejecución del entrenamiento.</p>
      </div>
    </div>
  </div>

  <hr>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            <li class="list-inline-item">
              <a href="https://github.com/jangel97/TFGNNSnakeFlappy" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          
            <li class="list-inline-item">
              <a href="https://www.youtube.com/watch?v=O6OZs0533bs&list=PLcgIGJXOlwfc7OrFi-RVXyNsvZ_g_fEvR" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fab fa-youtube fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
	  </ul>
          <p class="copyright text-muted">Creado por Jose Angle Morena Simón y Eduard Forés Ferrer</p>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="/static/vendor/jquery/jquery.min.js"></script>
  <script src="/static/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="/static/js/clean-blog.min.js"></script>

</body>

</html>
